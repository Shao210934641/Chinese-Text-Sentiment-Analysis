{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDHsgmglqD8U"
      },
      "source": [
        "# Chinese Text Sentiment Analysis Based on Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Label:***"
      ],
      "metadata": {
        "id": "UmV877bG0FJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$positive->label('美味')=1$$\n",
        "$$negative->label('难吃')=0$$"
      ],
      "metadata": {
        "id": "WVLgyBG5ea4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Flowchart:***\n",
        "<center><img width=500 src=\"https://i.imgur.com/Ln5oRY1.png\"></center>"
      ],
      "metadata": {
        "id": "NCNwvSMoczcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "D1icdxS5HTuy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "f1titB04qD8W"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import jieba  # jieba: Chinese words segmentation module\n",
        "from gensim.models import KeyedVectors  # gensim: load the pre-trained word vector\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import bz2  # unzip \n",
        "from sklearn.model_selection import train_test_split  # Split training and test samples\n",
        "# using tensorflow's keras interface\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained Word Vectors Model\n",
        "###### In the word vector model, each word is an index, which corresponds to a vector of length 300. Since the LSTM neural network model does not directly process Chinese text, we need to first perform word separation operations and then convert the words into word vectors."
      ],
      "metadata": {
        "id": "rVw81P8gHc4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place the downloaded word vectors in the embeddings folder in the root directory and unzip\n",
        "with open(\"embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
        "    decompressor = bz2.BZ2Decompressor()\n",
        "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
        "        new_file.write(decompressor.decompress(data))"
      ],
      "metadata": {
        "id": "tn8BuRvVv_J1"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "zoMND5sqqD8Y"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Chinese word separation embedding using gensim \n",
        "cn_model = KeyedVectors.load_word2vec_format('/content/embeddings/sgns.zhihu.bigram', binary=False, unicode_errors=\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "DPP3p0Y2qD8Z",
        "outputId": "4f505fab-cb4e-48b3-ef67-01da1a602203",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the word vector is 300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8.375510e-01,  4.208510e-01, -2.799180e-01,  5.622690e-01,\n",
              "       -8.997970e-01,  5.272690e-01,  9.496400e-01,  4.563700e-02,\n",
              "       -9.810400e-02, -2.630750e-01, -1.524345e+00,  7.652050e-01,\n",
              "       -1.986810e-01, -6.065320e-01,  5.041090e-01, -4.938500e-01,\n",
              "        5.872340e-01, -1.234050e-01, -4.244760e-01,  1.889090e-01,\n",
              "        7.961510e-01,  6.705470e-01,  7.920700e-02,  5.041190e-01,\n",
              "       -5.473530e-01, -6.844260e-01,  4.471700e-01, -2.957340e-01,\n",
              "       -2.229900e-01,  1.689930e-01,  3.720270e-01, -1.595700e-01,\n",
              "       -3.491800e-02,  2.385590e-01, -2.851160e-01, -3.496570e-01,\n",
              "       -2.146670e-01,  1.279030e-01,  1.851230e-01, -8.933700e-02,\n",
              "       -3.738830e-01, -3.261810e-01,  8.310600e-02, -2.764490e-01,\n",
              "        1.571830e-01, -6.430000e-03, -4.535830e-01,  3.596070e-01,\n",
              "       -4.212470e-01,  4.759860e-01,  1.874830e-01, -1.392510e-01,\n",
              "       -6.330000e-04,  1.503450e-01, -2.699750e-01, -3.896300e-02,\n",
              "       -9.544230e-01,  1.075360e-01, -5.690130e-01,  5.670360e-01,\n",
              "       -4.811400e-02,  6.133300e-01, -4.539180e-01, -5.689090e-01,\n",
              "        2.896540e-01,  6.357770e-01, -2.847990e-01,  7.098020e-01,\n",
              "        1.086606e+00, -3.694120e-01,  3.299900e-01,  3.011870e-01,\n",
              "       -4.837610e-01, -7.841770e-01,  2.162470e-01, -5.330670e-01,\n",
              "       -2.891000e-02,  3.623100e-02,  2.827150e-01,  6.735050e-01,\n",
              "        2.663360e-01,  9.530600e-02, -1.996720e-01, -6.727900e-02,\n",
              "       -1.855300e-02,  7.735000e-02, -5.930880e-01,  1.460950e-01,\n",
              "        1.235320e-01, -3.692230e-01,  3.521490e-01,  8.259170e-01,\n",
              "        4.163100e-02, -3.422400e-01, -2.160140e-01, -2.257000e-02,\n",
              "       -4.426690e-01, -5.237380e-01,  9.769520e-01,  9.761100e-02,\n",
              "        8.131600e-02,  1.249865e+00, -1.347290e-01, -4.944500e-02,\n",
              "        2.450660e-01, -9.516100e-02, -1.373600e-02,  2.589440e-01,\n",
              "       -2.307450e-01, -7.396090e-01, -1.387310e-01,  7.707160e-01,\n",
              "       -6.306910e-01, -7.997900e-02, -7.441800e-01,  5.895290e-01,\n",
              "       -8.884930e-01,  1.185760e-01, -4.050990e-01, -3.021340e-01,\n",
              "        5.811800e-01,  6.649350e-01,  1.378750e-01,  6.397900e-02,\n",
              "        9.010100e-02, -6.142100e-01,  3.680520e-01, -3.308950e-01,\n",
              "       -7.009300e-02, -3.792380e-01,  9.101310e-01,  6.782200e-02,\n",
              "       -5.490720e-01,  1.292290e-01,  1.560590e-01,  2.469130e-01,\n",
              "       -5.391900e-02,  3.840380e-01,  1.894790e-01,  2.606600e-02,\n",
              "       -6.332170e-01,  2.573600e-01,  1.967280e-01, -8.227460e-01,\n",
              "       -4.288110e-01, -3.262770e-01,  6.877430e-01,  7.053550e-01,\n",
              "       -8.101400e-02, -2.334680e-01,  6.219800e-02, -6.362430e-01,\n",
              "        1.933110e-01,  3.480220e-01,  4.008860e-01, -9.878700e-02,\n",
              "        1.902100e-01, -3.564020e-01, -1.772870e-01,  5.465770e-01,\n",
              "        2.861030e-01,  7.473600e-02, -4.498400e-01, -3.510960e-01,\n",
              "        4.962360e-01,  6.738700e-02, -9.542230e-01,  7.612400e-02,\n",
              "        1.124004e+00,  5.694360e-01, -1.693910e-01,  4.307700e-02,\n",
              "        1.049010e-01,  1.350860e-01, -2.019580e-01, -5.534500e-01,\n",
              "       -1.015365e+00,  1.169792e+00,  2.502820e-01,  1.975950e-01,\n",
              "       -6.416950e-01, -9.712700e-02,  5.445690e-01,  4.815240e-01,\n",
              "        6.051380e-01, -3.225670e-01, -5.030960e-01, -2.280690e-01,\n",
              "        5.468290e-01,  2.893320e-01, -4.626020e-01, -3.185000e-03,\n",
              "       -1.628060e-01, -1.683900e-01,  9.449500e-02, -6.396950e-01,\n",
              "        1.432850e-01, -1.008660e-01, -4.234660e-01,  3.131530e-01,\n",
              "        6.932200e-02,  1.691740e-01, -6.164500e-02, -7.792600e-01,\n",
              "        3.817640e-01, -2.610030e-01, -1.162530e-01,  6.449400e-02,\n",
              "       -8.123460e-01,  4.360900e-02, -4.961360e-01,  7.157490e-01,\n",
              "       -6.938460e-01,  2.178750e-01,  2.582620e-01,  3.909510e-01,\n",
              "        3.011010e-01,  9.815600e-02,  5.126400e-02,  7.179400e-01,\n",
              "        1.062510e-01,  4.506820e-01, -1.385290e-01, -7.843030e-01,\n",
              "       -1.643300e-02, -5.120510e-01,  7.013410e-01,  5.648950e-01,\n",
              "        1.832710e-01, -7.574150e-01, -4.377600e-01, -1.664640e-01,\n",
              "        1.199600e-01,  4.446600e-02, -7.481300e-01, -2.840190e-01,\n",
              "        3.475090e-01,  5.801960e-01,  2.461120e-01,  6.150700e-02,\n",
              "       -8.927140e-01, -6.825900e-02,  1.268098e+00,  1.025260e-01,\n",
              "        2.198650e-01, -4.548420e-01,  3.467400e-02,  1.387800e-01,\n",
              "        2.152560e-01, -1.520000e-04,  2.521600e-01, -5.611600e-02,\n",
              "        3.989510e-01, -4.545040e-01, -2.425520e-01,  1.634460e-01,\n",
              "       -8.490120e-01, -4.628340e-01,  1.050020e-01,  2.127820e-01,\n",
              "       -3.832050e-01, -1.274690e-01, -8.635690e-01,  9.936300e-02,\n",
              "       -3.554690e-01, -3.905850e-01,  1.786530e-01, -1.026720e-01,\n",
              "       -7.065050e-01,  8.223400e-02, -5.151900e-02, -9.509600e-02,\n",
              "       -3.977860e-01,  3.936270e-01,  3.537000e-03, -1.108840e-01,\n",
              "       -2.185610e-01, -2.296720e-01, -5.608100e-01, -5.568920e-01,\n",
              "       -5.222500e-02,  6.378300e-02,  5.635820e-01, -7.782280e-01,\n",
              "        5.836800e-01,  9.342290e-01, -4.287300e-02,  1.383850e-01,\n",
              "       -4.206350e-01,  2.797510e-01, -2.255450e-01,  2.723170e-01,\n",
              "       -6.376710e-01, -4.352860e-01, -9.615850e-01, -2.727510e-01,\n",
              "       -5.851680e-01,  1.902460e-01,  3.606010e-01, -8.082410e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "# Input a Chinese word and output a vector of words of length 300\n",
        "embedding_dim = cn_model['青岛'].shape[0]\n",
        "print('The length of the word vector is {}'.format(embedding_dim))\n",
        "cn_model['青岛']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "BJbNCXmPqD8a",
        "outputId": "0b9bcedd-b813-421b-b837-87cef91a17a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.55973804"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "# Calculate the cosine similarity of two words \n",
        "cn_model.similarity('英国', '伦敦')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "0MDlzj5iqD8a",
        "outputId": "1e28f86b-21e9-4288-df57-2fc330ff306f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('美元', 0.7081582546234131),\n",
              " ('港币', 0.6911839842796326),\n",
              " ('日元', 0.6810632348060608),\n",
              " ('汇率', 0.679571270942688),\n",
              " ('欧元', 0.6542581915855408),\n",
              " ('欧元和', 0.6420362591743469),\n",
              " ('英镑', 0.6394895911216736),\n",
              " ('贬值', 0.636674165725708),\n",
              " ('韩元', 0.6353235840797424),\n",
              " ('卢布', 0.6327826976776123)]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "# Input a Chinese word and find the 10 most similar words to it \n",
        "cn_model.most_similar(positive=['人民币'], topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "5O6aB9vYqD8a",
        "outputId": "5f80195a-185a-4207-a195-7316ff8a83e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In [英国 德国 法国 中国 帅哥]:\n",
            "The word that is not in the same category is: 帅哥\n"
          ]
        }
      ],
      "source": [
        "# Enter a set of Chinese words and find the word that is not in the same category\n",
        "test_words = '英国 德国 法国 中国 帅哥'\n",
        "test_words_result = cn_model.doesnt_match(test_words.split())\n",
        "print('In '+ '['+test_words+']' +':\\nThe word that is not in the same category is: %s' %test_words_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGdgVx_6qD8b"
      },
      "source": [
        "**datasets**  \n",
        "The datasets are placed in two separate .txt files: \n",
        "* <I>positive_samples.txt</I> (2000 Chinese positive reviews, label=1）\n",
        "* <I>negative_samples.txt</I> (2000 Chinese negative reviews, label=0）"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing "
      ],
      "metadata": {
        "id": "FBPtvFaLHxmU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "6tktc_wiqD8c"
      },
      "outputs": [],
      "source": [
        "train_texts_orig = []  # Store all reviews, one string per case\n",
        "train_target = []  # Store all labels. The first 2,000 reviews are positive, the last 2,000 are negative. \n",
        "with open(\"datasets/positive_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        dic = eval(line)\n",
        "        train_texts_orig.append(dic[\"text\"])\n",
        "        train_target.append(dic[\"label\"])\n",
        "\n",
        "with open(\"datasets/negative_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        dic = eval(line)\n",
        "        train_texts_orig.append(dic[\"text\"])\n",
        "        train_target.append(dic[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "XP_r6sKdqD8c",
        "outputId": "e81d9f61-fb11-4acc-e04f-d4998f1dadac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000\n",
            "4000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_texts_orig))\n",
        "print(len(train_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6qoPFriqD8d"
      },
      "source": [
        "**remove punctuation, jieba word segmentation and tokenize**  \n",
        "* First we remove the punctuation from each sample. \n",
        "* Then using jieba word segmentation, which returns a generator -> cut. \n",
        "* However, the generator cannot be tokenized directly, so we also need to convert it to a list -> cut_list. \n",
        "* Next, Index it. \n",
        "* Finally, the text evaluated in each case becomes a segment of indexed numbers corresponding to the words in the pre-trained word vectors model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "yebbnJxLqD8d"
      },
      "outputs": [],
      "source": [
        "train_tokens = []\n",
        "for text in train_texts_orig:\n",
        "    # Remove punctuation\n",
        "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
        "    # jieba word segmentation\n",
        "    cut = jieba.cut(text)  # jieba's output is a generator -> cut\n",
        "    cut_list = [ i for i in cut ]  # Convert the generator to list -> cut_list \n",
        "    for i, word in enumerate(cut_list):\n",
        "        try:\n",
        "            cut_list[i] = cn_model.key_to_index[word]  # Convert words to index\n",
        "        except KeyError:\n",
        "            cut_list[i] = 0  # If the word is not in the dictionary, output 0\n",
        "    train_tokens.append(cut_list)  # train_tokens is a long list containing 4000 small lists, corresponding to each evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BifCDXh_qD8d"
      },
      "source": [
        "**normalising index length**  \n",
        "Because each review is of a different length, it would be a waste of computing resources to simply take the longest one and fill the others to the same length, so we take a compromise length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "XmTm_YJRqD8d"
      },
      "outputs": [],
      "source": [
        "# Get lengths of all tokens\n",
        "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
        "num_tokens = np.array(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "2c8XtwTBqD8e",
        "outputId": "4caca192-d70b-4804-bd27-067a62c54fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.42575"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "# The average length of tokens\n",
        "np.mean(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "wm7wIO5PqD8e",
        "outputId": "e7a85331-20b2-4822-c8a5-71b2f2667427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1540"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "# The longest tokens length\n",
        "np.max(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "1dDtvqirqD8e",
        "outputId": "3e2808c7-c6fb-489e-beda-9fc149c9134f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Display the distribution of tokens length \n",
        "plt.hist(np.log(num_tokens), bins = 100)\n",
        "plt.xlim((0,10))\n",
        "plt.ylabel('number of tokens')\n",
        "plt.xlabel('length of tokens')\n",
        "plt.title('Distribution of tokens length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "Y0G70gx1qD8f",
        "outputId": "5b6a2dae-d2b4-4c1d-bcf0-56b07d0910d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "236"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "# Take the mean of tokens + standard deviation of tokens *2. The distribution of tokens lengths is approximated as a normal distribution\n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "63mGmdP-qD8f",
        "outputId": "18e9d4b5-03ef-47b4-fbce-252ced9ccb77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9565"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "# When tokens are taken to be 236 in length, about 95% of the sample is covered. \n",
        "# (Under-length tokens will be padding and over-length tokens will be trimmed later)\n",
        "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rQAiWmqD8f"
      },
      "source": [
        "**reverse tokenize**  \n",
        "Define a function to convert the index into readable text, which is important for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "1tZ0_f5CqD8f"
      },
      "outputs": [],
      "source": [
        "# Convert tokens to text\n",
        "def reverse_tokens(tokens):\n",
        "    text = ''\n",
        "    for i in tokens:\n",
        "        if i != 0:\n",
        "#             text = text + cn_model.index2word[i]\n",
        "            text = text + cn_model.index_to_key[i]\n",
        "        else:\n",
        "            text = text + ' '\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "zCRvjZ75qD8g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "fe5ed28c-109f-405e-de7b-9a12ba73dc80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "# After tokenize and revert to text, the punctuation is gone\n",
        "reverse = reverse_tokens(train_tokens[0])\n",
        "reverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "pafagz2TqD8g",
        "outputId": "33e7f653-cd1e-4d16-d081-e8cb3d0c40b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "# original text\n",
        "train_texts_orig[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Embedding Matrix "
      ],
      "metadata": {
        "id": "46SjJzasIGD1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhSQX7a7qD8h"
      },
      "source": [
        "**create the embedding matrix**  \n",
        "* Now, we need to create an embedding matrix. According to keras, we need to prepare a dimension of $(numwords, embedddingdim)$ of the matrix, where numwords represents the number of words we use, and emdeddingdim is 300 in the pre-trained word vector model we are using now, with each word represented by a vector of length 300.\n",
        "* Note that we use only the first 50k most frequently used words. There are 2.6 million words in this pre-trained word vector model, and it would be a waste of computational resources to use them all for the classification problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "HIe1dxTyqD8h",
        "outputId": "7a3ce60d-ff3c-439e-ecf0-e0b9146a7a02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "source": [
        "embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "ubuNjcfCqD8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf4a573-5691-4624-a0ff-60ad5e94d39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of embedding_matrix: (50000, 300)\n"
          ]
        }
      ],
      "source": [
        "num_words = 50000  # Use only the first 50,000 words\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))  # Initialise embedding_matrix\n",
        "for i in range(num_words):\n",
        "    embedding_matrix[i,:] = cn_model[cn_model.index_to_key[i]]\n",
        "embedding_matrix = embedding_matrix.astype('float32')  # embedding_matrix is a matrix of [num_words, embedding_dim]. \n",
        "print('Dimension of embedding_matrix:', embedding_matrix.shape)  # Dimension of embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDojFDN6qD8i"
      },
      "source": [
        "**padding and truncating**  \n",
        "After we convert the text into indexes, the length of each string of indexes is not equal, so in order to facilitate the training of the model, we need to normalise the length of the indexes. Above, we have chosen 236 samples, which can cover 95% of the length of the training samples.  \n",
        "Next, we do padding and truncating. We generally use the 'pre' method, which will fill in the front of the text indexes with 0. Because, according to some research materials in practice, if you fill in the text indexes after 0, it will have some adverse effects on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "2gd0fpAwqD8i"
      },
      "outputs": [],
      "source": [
        "# Padding and truncating. The input train_tokens is a list, and the returned train_pad is a numpy array. \n",
        "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
        "                            padding='pre', truncating='pre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "jUckArrrqD8i"
      },
      "outputs": [],
      "source": [
        "# Words beyond the 50,000 word vector are replaced by 0\n",
        "train_pad[ train_pad>=num_words ] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "YZVwlTZjqD8i",
        "outputId": "bb8c3929-7946-437a-d520-6ff4f4eef4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "         290,  3053,    57,   169,    73,     1,    25, 11216,    49,\n",
              "         163, 15985,     0,     0,    30,     8,     0,     1,   228,\n",
              "         223,    40,    35,   653,     0,     5,  1642,    29, 11216,\n",
              "        2751,   500,    98,    30,  3159,  2225,  2146,   371,  6285,\n",
              "         169, 27396,     1,  1191,  5432,  1080, 20055,    57,   562,\n",
              "           1, 22671,    40,    35,   169,  2567,     0, 42665,  7761,\n",
              "         110,     0,     0, 41281,     0,   110,     0, 35891,   110,\n",
              "           0, 28781,    57,   169,  1419,     1, 11670,     0, 19470,\n",
              "           1,     0,     0,   169, 35071,    40,   562,    35, 12398,\n",
              "         657,  4857], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "# Fill in the front of the text index with 0. Text at the end\n",
        "train_pad[33]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "-Rbp1zX1qD8j"
      },
      "outputs": [],
      "source": [
        "# create target labels, 1 for the first 2000 samples, 0 for the next 2000\n",
        "train_target = np.array(train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "I6cFcpv8qD8j"
      },
      "outputs": [],
      "source": [
        "# 90% for training, 10% for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.1, random_state=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building "
      ],
      "metadata": {
        "id": "uTDWSQICHN7d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdSsCmJJqD8k"
      },
      "source": [
        "Now we build the LSTM model using keras. The first layer of the model is the embedding layer, and only after we have transformed the token index into a word vector matrix can the text be processed by the neural network. Keras provides an embedding interface to avoid tedious sparse matrix operations.   \n",
        "The matrix entered in the Embedding layer is: $$(batchsize, maxtokens)$$\n",
        "The output matrix is: $$(batchsize, maxtokens, embeddingdim)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "zXxMebE0qD8k"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "H05FWesfqD8k"
      },
      "outputs": [],
      "source": [
        "# The first layer of the model is embedding\n",
        "model.add(Embedding(num_words,\n",
        "                    embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_tokens,\n",
        "                    trainable=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "Knl_DEIpqD8m"
      },
      "outputs": [],
      "source": [
        "# 有兴趣的同学可以调整一下模型参数, 看看会不会有更好的结果\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "model.add(LSTM(units=16, return_sequences=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQlyH9mNqD8m"
      },
      "source": [
        "We tried several neural network structures, and because the training samples were relatively small, the training process did not take long to complete.\n",
        "* GRU: The test samples could achieve 87% accuracy if GRU was used. However, when I tested my own text content, I found that the output of the last layer of the GRU activation function was all around 0.5, indicating that the model's judgement was not very clear, and after testing, I found that the model sometimes missed the judgement for negative sentences. We would expect the output to be close to 0 for negative samples and close to 1 for positive samples rather than hovering between 0.5.\n",
        "* Single LSTM, stacked LSTM and BiLSTM: We tested both LSTM and BiLSTM and found that BiLSTM performed best, with LSTM performing slightly better than GRU, probably because BiLSTM has better memory for longer sentence structures.\n",
        "* After Embedding the first layer we used BiLSTM to return sequences, then the second layer of 16 units of LSTM did not return sequences but only the final result, and finally a fully linked layer with a sigmoid activation function to output the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAJgtuqIqD8m"
      },
      "outputs": [],
      "source": [
        "# GRU code\n",
        "# model.add(GRU(units=32, return_sequences=True))\n",
        "# model.add(GRU(units=16, return_sequences=True))\n",
        "# model.add(GRU(units=4, return_sequences=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "7X2TMhweqD8m"
      },
      "outputs": [],
      "source": [
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(lr=1e-3)"
      ],
      "metadata": {
        "id": "tPToqkhSP0nN"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "BxiEHtqlqD8m"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "bRaYlNQLqD8n",
        "outputId": "bfb12e00-8688-4559-aaa3-9f898940d98d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 236, 300)          15000000  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 236, 128)          186880    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 16)                9280      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 15,196,177\n",
            "Trainable params: 196,177\n",
            "Non-trainable params: 15,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training "
      ],
      "metadata": {
        "id": "9bB4w7ssHH4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "7Mobk-LHqD8n"
      },
      "outputs": [],
      "source": [
        "# Create a storage point for the weights\n",
        "path_checkpoint = 'sentiment_checkpoint.keras'\n",
        "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
        "                                      verbose=1, save_weights_only=True,\n",
        "                                      save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "5f5fcwR7qD8n"
      },
      "outputs": [],
      "source": [
        "# Try to load a trained model\n",
        "try:\n",
        "    model.load_weights(path_checkpoint)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "UkDA3dOBqD8n"
      },
      "outputs": [],
      "source": [
        "# Define early stopping. If the validation loss does not improve within 5 epochs then stop training\n",
        "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "7hFmjYzBqD8o"
      },
      "outputs": [],
      "source": [
        "# Automatic learning rate reduction\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
        "                                       verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "PvSOKfxBqD8o"
      },
      "outputs": [],
      "source": [
        "# Define the callback function\n",
        "callbacks = [\n",
        "    earlystopping, \n",
        "    checkpoint,\n",
        "    lr_reduction\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "scrolled": false,
        "id": "fGSxzYcEqD8o",
        "outputId": "3e7444e2-279f-431b-9dc9-7404524144e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "26/26 [==============================] - 63s 2s/step - loss: 0.3802 - accuracy: 0.8423 - val_loss: 0.3791 - val_accuracy: 0.8528\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.37912, saving model to sentiment_checkpoint.keras\n",
            "Epoch 2/20\n",
            "26/26 [==============================] - 55s 2s/step - loss: 0.2996 - accuracy: 0.8821 - val_loss: 0.3279 - val_accuracy: 0.8611\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.37912 to 0.32787, saving model to sentiment_checkpoint.keras\n",
            "Epoch 3/20\n",
            "26/26 [==============================] - 55s 2s/step - loss: 0.2826 - accuracy: 0.8917 - val_loss: 0.3112 - val_accuracy: 0.8722\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.32787 to 0.31125, saving model to sentiment_checkpoint.keras\n",
            "Epoch 4/20\n",
            "26/26 [==============================] - 55s 2s/step - loss: 0.2770 - accuracy: 0.8917 - val_loss: 0.3519 - val_accuracy: 0.8583\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.31125\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 5/20\n",
            "26/26 [==============================] - 58s 2s/step - loss: 0.2366 - accuracy: 0.9136 - val_loss: 0.3224 - val_accuracy: 0.8694\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.31125\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 6/20\n",
            "26/26 [==============================] - 71s 3s/step - loss: 0.2201 - accuracy: 0.9225 - val_loss: 0.3160 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.31125\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 7/20\n",
            "26/26 [==============================] - 63s 2s/step - loss: 0.2181 - accuracy: 0.9235 - val_loss: 0.3156 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.31125\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 8/20\n",
            "26/26 [==============================] - 54s 2s/step - loss: 0.2180 - accuracy: 0.9235 - val_loss: 0.3156 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.31125\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "Epoch 00008: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fce1aa4d490>"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ],
      "source": [
        "# Start training \n",
        "model.fit(X_train, y_train,\n",
        "          validation_split=0.1, \n",
        "          epochs=20,\n",
        "          batch_size=128,\n",
        "          callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Test"
      ],
      "metadata": {
        "id": "R7ctB_aHG-hc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf5yKPeLqD8o"
      },
      "source": [
        "We start by making predictions on the test sample and get a good accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "GLwYn1xkqD8o",
        "outputId": "58f508fb-1c98-4fa9-abd0-15ddfbb7766c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 2s 159ms/step - loss: 0.3188 - accuracy: 0.8700\n",
            "Accuracy:87.00%\n"
          ]
        }
      ],
      "source": [
        "result = model.evaluate(X_test, y_test)\n",
        "print('Accuracy:{0:.2%}'.format(result[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define a prediction function to predict the label of the input text. The results show that the model is accurate for negative sentences and simple logical structures."
      ],
      "metadata": {
        "id": "ws_uCSrXmzg4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "NHOoWluNqD8p"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text):\n",
        "    print(text, end=\" \")\n",
        "    # remove punctuation\n",
        "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
        "    # word segmentation\n",
        "    cut = jieba.cut(text)\n",
        "    cut_list = [ i for i in cut ]\n",
        "    # tokenize\n",
        "    for i, word in enumerate(cut_list):\n",
        "        try:\n",
        "            # cut_list[i] = cn_model.vocab[word].index\n",
        "            cut_list[i] = cn_model.key_to_index[word]\n",
        "            if cut_list[i] >= 50000:\n",
        "                cut_list[i] = 0\n",
        "        except KeyError:\n",
        "            cut_list[i] = 0\n",
        "    # padding\n",
        "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
        "                           padding='pre', truncating='pre')\n",
        "    # predict \n",
        "    result = model.predict(x=tokens_pad)\n",
        "    coef = result[0][0]\n",
        "    if coef >= 0.5:\n",
        "        # print('-> prediction: positive','-> output=%.2f'%coef)\n",
        "        print('-> output=%.2f'%coef, '-> positive')\n",
        "    else:\n",
        "        print('-> output=%.2f'%coef, '-> negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "crkPH1PuqD8p",
        "outputId": "c3f596ad-da6d-4fe0-fbe8-a08796494744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "酒店设施不是新的，服务态度很不好 -> output=0.15 -> negative\n",
            "酒店卫生条件非常不好 -> output=0.12 -> negative\n",
            "床铺非常舒适 -> output=0.65 -> positive\n",
            "房间很凉，不给开暖气 -> output=0.18 -> negative\n",
            "房间很凉爽，空调冷气很足 -> output=0.52 -> positive\n",
            "酒店环境不好，住宿体验很不好 -> output=0.08 -> negative\n",
            "房间隔音不到位 -> output=0.27 -> negative\n",
            "晚上回来发现没有打扫卫生 -> output=0.19 -> negative\n",
            "因为过节所以要我临时加钱，比团购的价格贵 -> output=0.11 -> negative\n"
          ]
        }
      ],
      "source": [
        "test_list = [\n",
        "    '酒店设施不是新的，服务态度很不好',\n",
        "    '酒店卫生条件非常不好',\n",
        "    '床铺非常舒适',\n",
        "    '房间很凉，不给开暖气',\n",
        "    '房间很凉爽，空调冷气很足',\n",
        "    '酒店环境不好，住宿体验很不好',\n",
        "    '房间隔音不到位' ,\n",
        "    '晚上回来发现没有打扫卫生',\n",
        "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
        "]\n",
        "for text in test_list:\n",
        "    predict_sentiment(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Analysis "
      ],
      "metadata": {
        "id": "679sGH3AG5ap"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av_ceAdrqD8p"
      },
      "source": [
        "Through our analysis, we find that the meaning of the misclassified text is mostly ambiguous, and even humans can not easily determine the polarity. For example, this sentence with index 305 seems to have no element of satisfaction at all, but this example rating is marked as positive in the training sample, and the prediction of a negative rating made by our model seems reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "JmlVKiYkqD8p"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = y_pred.T[0]\n",
        "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
        "y_pred = np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "dE_-OBr0qD8p"
      },
      "outputs": [],
      "source": [
        "y_actual = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "Z_HEZ9z_qD8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7787cd2-7929-4191-debf-9326d4c362d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The misclassified indexes are as follows:\n",
            " [  1   5   7  14  15  21  30  52  59  72  73  80  81  90 101 118 123 127\n",
            " 128 130 135 138 148 156 182 186 189 207 211 215 218 223 228 238 246 253\n",
            " 258 267 283 297 302 305 311 316 335 342 352 365 373 390 391 394]\n"
          ]
        }
      ],
      "source": [
        "# indexes of misclassifications\n",
        "misclassified = np.where( y_pred != y_actual )[0]\n",
        "print('The misclassified indexes are as follows:\\n', misclassified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "scrolled": true,
        "id": "ZQZq5avDqD8q",
        "outputId": "762211a3-5cdb-44a3-a031-9c54ef2c3280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of misclassifications is: 52\n"
          ]
        }
      ],
      "source": [
        "# total number of misclassifications\n",
        "print('The total number of misclassifications is:', len(misclassified))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**output some misclassified samples**"
      ],
      "metadata": {
        "id": "WGseScFcSbMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "7srmaSxnqD8q",
        "outputId": "4b13eb27-b054-4b28-89b1-24b3ca89ad93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction 0\n",
            "groundtruth 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                            非常感谢携程  的客服人员帮助 了由客人自己信用卡担保的失误没有及时告知的事件下次有机会会再次光临本宾馆支持携程'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 203
        }
      ],
      "source": [
        "idx=305\n",
        "print('prediction', y_pred[idx])\n",
        "print('groundtruth', y_actual[idx])\n",
        "reverse_tokens(X_test[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "VSSJZLdQqD8q",
        "outputId": "a96214ff-e59c-4f59-fe7f-7634cfeb90d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction 1\n",
            "groundtruth 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                               服务员太差房间很吵隔音不好房间味道很重很一般不如稀土国际大酒店好收费有低还是 服务很好离火车站方便'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "idx=394\n",
        "print('prediction', y_pred[idx])\n",
        "print('groundtruth', y_actual[idx])\n",
        "reverse_tokens(X_test[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "SIE0A9mmqD8q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "61e1fecf-d71e-4e51-b714-25d17feb5d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction 1\n",
            "groundtruth 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                 招待所而已吧唯一可取之处就是交通还行三环边上'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "source": [
        "idx=15\n",
        "print('prediction', y_pred[idx])\n",
        "print('groundtruth', y_actual[idx])\n",
        "reverse_tokens(X_test[idx])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx=21\n",
        "print('prediction', y_pred[idx])\n",
        "print('groundtruth', y_actual[idx])\n",
        "reverse_tokens(X_test[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "6SjDyHYhSr2h",
        "outputId": "24a8f436-289d-413f-dea0-d92ff8d540be"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction 1\n",
            "groundtruth 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                      入住这个酒店应该算是被携程给误导了把它放在了第一选择位置如果酒店介绍加上 说明“  ”估计生意 了 还有  好好感谢携程'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1AXLJX_KV26U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QVV5kR9VV29C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d0Z_23cgV2_X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}