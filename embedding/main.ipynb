{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Text Sentiment Analysis Based on Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$label('美味')=1 （translation: label('delicious')=1$$\n",
    "$$label('难吃')=0 （translation: label('awful')=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba  # jieba: Chinese words segmentation module\n",
    "from gensim.models import KeyedVectors  # gensim: load the pre-trained word vector\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import bz2  # unzip \n",
    "# using tensorflow's keras interface\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place the downloaded word vectors in the embeddings folder in the root directory and unzip\n",
    "with open(\"embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained Chinese word separation embedding using gensim \n",
    "cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词向量模型**  \n",
    "在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量，我们今天需要构建的LSTM神经网络模型并不能直接处理汉字文本，需要先进行分次并把词汇转换为词向量，步骤请参考下图，步骤的讲解会跟着代码一步一步来，如果你不知道RNN，GRU，LSTM是什么，我推荐deeplearning.ai的课程，网易公开课有免费中文字幕版，但我还是推荐有习题和练习代码部分的的coursera原版：  \n",
    "<img src='flowchart.jpg' style='width:400px;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the word vector is 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8.375510e-01,  4.208510e-01, -2.799180e-01,  5.622690e-01,\n",
       "       -8.997970e-01,  5.272690e-01,  9.496400e-01,  4.563700e-02,\n",
       "       -9.810400e-02, -2.630750e-01, -1.524345e+00,  7.652050e-01,\n",
       "       -1.986810e-01, -6.065320e-01,  5.041090e-01, -4.938500e-01,\n",
       "        5.872340e-01, -1.234050e-01, -4.244760e-01,  1.889090e-01,\n",
       "        7.961510e-01,  6.705470e-01,  7.920700e-02,  5.041190e-01,\n",
       "       -5.473530e-01, -6.844260e-01,  4.471700e-01, -2.957340e-01,\n",
       "       -2.229900e-01,  1.689930e-01,  3.720270e-01, -1.595700e-01,\n",
       "       -3.491800e-02,  2.385590e-01, -2.851160e-01, -3.496570e-01,\n",
       "       -2.146670e-01,  1.279030e-01,  1.851230e-01, -8.933700e-02,\n",
       "       -3.738830e-01, -3.261810e-01,  8.310600e-02, -2.764490e-01,\n",
       "        1.571830e-01, -6.430000e-03, -4.535830e-01,  3.596070e-01,\n",
       "       -4.212470e-01,  4.759860e-01,  1.874830e-01, -1.392510e-01,\n",
       "       -6.330000e-04,  1.503450e-01, -2.699750e-01, -3.896300e-02,\n",
       "       -9.544230e-01,  1.075360e-01, -5.690130e-01,  5.670360e-01,\n",
       "       -4.811400e-02,  6.133300e-01, -4.539180e-01, -5.689090e-01,\n",
       "        2.896540e-01,  6.357770e-01, -2.847990e-01,  7.098020e-01,\n",
       "        1.086606e+00, -3.694120e-01,  3.299900e-01,  3.011870e-01,\n",
       "       -4.837610e-01, -7.841770e-01,  2.162470e-01, -5.330670e-01,\n",
       "       -2.891000e-02,  3.623100e-02,  2.827150e-01,  6.735050e-01,\n",
       "        2.663360e-01,  9.530600e-02, -1.996720e-01, -6.727900e-02,\n",
       "       -1.855300e-02,  7.735000e-02, -5.930880e-01,  1.460950e-01,\n",
       "        1.235320e-01, -3.692230e-01,  3.521490e-01,  8.259170e-01,\n",
       "        4.163100e-02, -3.422400e-01, -2.160140e-01, -2.257000e-02,\n",
       "       -4.426690e-01, -5.237380e-01,  9.769520e-01,  9.761100e-02,\n",
       "        8.131600e-02,  1.249865e+00, -1.347290e-01, -4.944500e-02,\n",
       "        2.450660e-01, -9.516100e-02, -1.373600e-02,  2.589440e-01,\n",
       "       -2.307450e-01, -7.396090e-01, -1.387310e-01,  7.707160e-01,\n",
       "       -6.306910e-01, -7.997900e-02, -7.441800e-01,  5.895290e-01,\n",
       "       -8.884930e-01,  1.185760e-01, -4.050990e-01, -3.021340e-01,\n",
       "        5.811800e-01,  6.649350e-01,  1.378750e-01,  6.397900e-02,\n",
       "        9.010100e-02, -6.142100e-01,  3.680520e-01, -3.308950e-01,\n",
       "       -7.009300e-02, -3.792380e-01,  9.101310e-01,  6.782200e-02,\n",
       "       -5.490720e-01,  1.292290e-01,  1.560590e-01,  2.469130e-01,\n",
       "       -5.391900e-02,  3.840380e-01,  1.894790e-01,  2.606600e-02,\n",
       "       -6.332170e-01,  2.573600e-01,  1.967280e-01, -8.227460e-01,\n",
       "       -4.288110e-01, -3.262770e-01,  6.877430e-01,  7.053550e-01,\n",
       "       -8.101400e-02, -2.334680e-01,  6.219800e-02, -6.362430e-01,\n",
       "        1.933110e-01,  3.480220e-01,  4.008860e-01, -9.878700e-02,\n",
       "        1.902100e-01, -3.564020e-01, -1.772870e-01,  5.465770e-01,\n",
       "        2.861030e-01,  7.473600e-02, -4.498400e-01, -3.510960e-01,\n",
       "        4.962360e-01,  6.738700e-02, -9.542230e-01,  7.612400e-02,\n",
       "        1.124004e+00,  5.694360e-01, -1.693910e-01,  4.307700e-02,\n",
       "        1.049010e-01,  1.350860e-01, -2.019580e-01, -5.534500e-01,\n",
       "       -1.015365e+00,  1.169792e+00,  2.502820e-01,  1.975950e-01,\n",
       "       -6.416950e-01, -9.712700e-02,  5.445690e-01,  4.815240e-01,\n",
       "        6.051380e-01, -3.225670e-01, -5.030960e-01, -2.280690e-01,\n",
       "        5.468290e-01,  2.893320e-01, -4.626020e-01, -3.185000e-03,\n",
       "       -1.628060e-01, -1.683900e-01,  9.449500e-02, -6.396950e-01,\n",
       "        1.432850e-01, -1.008660e-01, -4.234660e-01,  3.131530e-01,\n",
       "        6.932200e-02,  1.691740e-01, -6.164500e-02, -7.792600e-01,\n",
       "        3.817640e-01, -2.610030e-01, -1.162530e-01,  6.449400e-02,\n",
       "       -8.123460e-01,  4.360900e-02, -4.961360e-01,  7.157490e-01,\n",
       "       -6.938460e-01,  2.178750e-01,  2.582620e-01,  3.909510e-01,\n",
       "        3.011010e-01,  9.815600e-02,  5.126400e-02,  7.179400e-01,\n",
       "        1.062510e-01,  4.506820e-01, -1.385290e-01, -7.843030e-01,\n",
       "       -1.643300e-02, -5.120510e-01,  7.013410e-01,  5.648950e-01,\n",
       "        1.832710e-01, -7.574150e-01, -4.377600e-01, -1.664640e-01,\n",
       "        1.199600e-01,  4.446600e-02, -7.481300e-01, -2.840190e-01,\n",
       "        3.475090e-01,  5.801960e-01,  2.461120e-01,  6.150700e-02,\n",
       "       -8.927140e-01, -6.825900e-02,  1.268098e+00,  1.025260e-01,\n",
       "        2.198650e-01, -4.548420e-01,  3.467400e-02,  1.387800e-01,\n",
       "        2.152560e-01, -1.520000e-04,  2.521600e-01, -5.611600e-02,\n",
       "        3.989510e-01, -4.545040e-01, -2.425520e-01,  1.634460e-01,\n",
       "       -8.490120e-01, -4.628340e-01,  1.050020e-01,  2.127820e-01,\n",
       "       -3.832050e-01, -1.274690e-01, -8.635690e-01,  9.936300e-02,\n",
       "       -3.554690e-01, -3.905850e-01,  1.786530e-01, -1.026720e-01,\n",
       "       -7.065050e-01,  8.223400e-02, -5.151900e-02, -9.509600e-02,\n",
       "       -3.977860e-01,  3.936270e-01,  3.537000e-03, -1.108840e-01,\n",
       "       -2.185610e-01, -2.296720e-01, -5.608100e-01, -5.568920e-01,\n",
       "       -5.222500e-02,  6.378300e-02,  5.635820e-01, -7.782280e-01,\n",
       "        5.836800e-01,  9.342290e-01, -4.287300e-02,  1.383850e-01,\n",
       "       -4.206350e-01,  2.797510e-01, -2.255450e-01,  2.723170e-01,\n",
       "       -6.376710e-01, -4.352860e-01, -9.615850e-01, -2.727510e-01,\n",
       "       -5.851680e-01,  1.902460e-01,  3.606010e-01, -8.082410e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input a Chinese word and output a vector of words of length 300\n",
    "embedding_dim = cn_model['青岛'].shape[0]\n",
    "print('The length of the word vector is {}'.format(embedding_dim))\n",
    "cn_model['青岛']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity for Vector Space Models by Christian S. Perone\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55973804"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the similarity of two words\n",
    "cn_model.similarity('英国', '伦敦')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('美元', 0.7081581354141235),\n",
       " ('港币', 0.6911839246749878),\n",
       " ('日元', 0.6810634136199951),\n",
       " ('汇率', 0.679571270942688),\n",
       " ('欧元', 0.6542582511901855),\n",
       " ('欧元和', 0.6420362591743469),\n",
       " ('英镑', 0.6394896507263184),\n",
       " ('贬值', 0.636674165725708),\n",
       " ('韩元', 0.6353235840797424),\n",
       " ('卢布', 0.6327827572822571)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input a Chinese word and find the 10 most similar words to it \n",
    "cn_model.most_similar(positive=['人民币'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [英国 德国 法国 中国 帅哥]:\n",
      "The word that is not in the same category is: 帅哥\n"
     ]
    }
   ],
   "source": [
    "# enter a set of Chinese words and find the word that is not in the same category\n",
    "test_words = '英国 德国 法国 中国 帅哥'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('In '+ '['+test_words+']' +':\\nThe word that is not in the same category is: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练语料**  \n",
    "本教程使用了谭松波老师的酒店评论语料，即使是这个语料也很难找到下载链接，在某博客还得花积分下载，而我不知道怎么赚取积分，后来好不容易找到一个链接但竟然是失效的，再后来尝试把链接粘贴到迅雷上终于下载了下来，希望大家以后多多分享资源。  \n",
    "训练样本分别被放置在两个文件夹里：\n",
    "分别的pos和neg，每个文件夹里有2000个txt文件，每个文件内有一段评语，共有4000个训练样本，这样大小的样本数据在NLP中属于非常迷你的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "# 分别为 正面评价'pos'文件夹 和 负面评价'neg'文件夹\n",
    "# 每个文件夹中有2000个txt文件，每个文件中是一例评价\n",
    "import os\n",
    "pos_txts = os.listdir('pos')\n",
    "neg_txts = os.listdir('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 4000\n"
     ]
    }
   ],
   "source": [
    "print( '样本总共: '+ str(len(pos_txts) + len(neg_txts)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行简单的预处理, 以避免乱码\n",
    "train_texts_orig = []\n",
    "train_target = []\n",
    "with open(\"positive_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])\n",
    "\n",
    "with open(\"negative_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 我们使用tensorflow的keras接口来建模\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "# from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.python.keras.optimizers import RMSprop\n",
    "# from tensorflow.python.keras.optimizers import Adam\n",
    "# from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词和tokenize**  \n",
    "首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wl/0x1mzt_901j9znqzbv5pn4p40000gn/T/jieba.cache\n",
      "Loading model cost 0.461 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "#             cut_list[i] = cn_model.vocab[word].index\n",
    "            cut_list[i] = cn_model.key_to_index[word]\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引长度标准化**  \n",
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.42575"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdwklEQVR4nO3deZhdVZ3u8e9rEJAZOgEhIRRoFJF2jIqCLTaoCCjcqyAoGCZpWxsU6KtBUMQr19jaXEfajkwRERkcQHBCBtFmMowBkZYLASKRhDmgIsH3/rFXkZOiKnvXcOqcqno/z3Oec/baw/rVqarzO2utvdeWbSIiIlblOZ0OICIiul+SRURE1EqyiIiIWkkWERFRK8kiIiJqJVlEREStJItoTNI3JH1yhI41XdLjkiaV5cslHTISxy7H+4mkWSN1vEHU+1lJD0j64wgca0dJi0YiriHWf4CkX3eo7tMlfbYTdUf/kiwCAEkLJf1Z0jJJj0i6UtIHJT3zN2L7g7b/d8Nj7byqbWzfY3sd20+PQOyflvTtPsd/u+15wz32IOPYHDgK2Mb28/tZ39EP/27VyaQUzSVZRKt32F4X2AKYA3wcOGWkK5G02kgfs0tsATxoe0mnA4kYaUkW8Sy2H7V9AfAeYJakbWHlrgFJkyVdWFohD0n6laTnSDoDmA78qHQzfUxSjyRLOljSPcClLWWtieMFkq6V9Kik8yVtVOp61jfy3taLpF2ATwDvKfXdVNY/061V4jpW0t2Slkj6lqT1y7reOGZJuqd0IR0z0Hsjaf2y/9JyvGPL8XcGLgY2K3Gc3me/tYGftKx/XNJmktaQ9CVJ95XHlyStMUDdh0v6raRpZb8vlpjvL12Ez2t9vyQdVX7exZIObDnOruU4yyT9QdK/rvIPYsV+W0u6uPy+b5e0d8u60yV9XdJF5bjXSHpBy/q3ln0elXSSpF9KOkTSS4BvAK8v78kjLVVuONDxYvQlWcSAbF8LLALe2M/qo8q6KcAmVB/Ytr0/cA9VK2Ud2//Wss+bgJcAbxugyvcDBwGbAcuBrzSI8afA/wHOLvW9vJ/NDiiPNwNbAesAX+uzzQ7Ai4GdgE+VD7H+fBVYvxznTSXmA23/Ang7cF+J44A+cT7RZ/06tu8DjgG2A14BvBx4LXBs30pVjRUdALzJ9iLg88CLyn4vBKYCn2rZ5fklzqnAwcDXJW1Y1p0C/FNpRW4LXDrAz9pa/9pUyfA7wMbAvsBJkl7astm+wPHAhsAdwAll38nAecDRwN8BtwNvKO/LbcAHgavKe7JB3fGiM5Isos59wEb9lD8FbApsYfsp279y/URjn7b9hO0/D7D+DNu3lA/WTwJ7qwyAD9P7gBNt32n7caoPrX36tGqOt/1n2zcBN1F9cK+kxPIe4Gjby2wvBP4d2H+YsX3G9hLbS6k+HFuPJ0knUiXYN9teKknAB4AjbD9kexlVwtynZb+nynGfsv1j4HGqZNi7bhtJ69l+2Pb1DeLcHVho+zTby8s+3wPe3bLN921fa3s5cCZVIgPYFbjV9vfLuq8ATU4AGOh40QFJFlFnKvBQP+VfoPq293NJd0qa3eBY9w5i/d3Ac4HJjaJctc3K8VqPvRpVi6hX64fXn6haH31NBlbv51hTRzi2zVqWNwAOBT5n+9FSNgVYC7iudAM+Avy0lPd6sHzI9mr9md5F9QF+d+kOen2DOLcAXtdbX6nzfVQtmF4DvYeb0fK7LV8qmgz0N/mdxChJsogBSXoN1Qfhs85UKd+sj7K9FfAO4EhJO/WuHuCQdS2PzVteT6f6BvwA8ATVh2NvXJNY+YOx7rj3UX3YtR57OXB/zX59PVBi6nusPzTcv784+4vtvpblh6m+1Z8mafuWOP4MvNT2BuWxvu1GH6a2f2N7D6rupB8C5zTY7V7gly31bVC6jf65wb6LgWm9C6VlNK1lfaa+HgOSLOJZJK0naXfgu8C3bS/oZ5vdJb2w/OM/BjxdHlB9CG81hKr3k7SNpLWAzwDnlVNr/xtYU9Jukp5L1affOgh8P9CjltN8+zgLOELSlpLWYcUYx/IBtu9XieUc4ARJ60raAjgS+Paq91wpzr/rHVxvie1YSVNK3/6n+h7P9uVU3+J/IOl1tv8GfBP4v5I2BpA0VdJAY0HPkLS6pPdJWt/2U6z43dW5EHiRpP0lPbc8XrOKsZ1WFwF/L2nP0vX3YVZukdwPTJO0eoNjRYckWUSrH0laRvUt8hjgRODAAbadAfyCqi/8KuCk8qEG8DmqD8BHmp5pU5wBnE7V/bAmcDhUZ2cBHwJOpvoW/wQrd2OcW54flNRf//up5dhXAHcBfwEOG0RcrQ4r9d9J1eL6Tjl+Ldu/o0oOd5b3ZjPgs8B84GZgAXB9Keu778VUv4sLJL2a6rTmO4CrJT1G9bt4cd/9BrA/sLDs90FgvwaxLwPeSjUuch/V7+jzrJy0B9r3AWAv4N+AB4FtqH7mJ8smlwK3An+U9EDDnyFGmXLzo4gYTaUFuAh4n+3LOh1PNJOWRUS0naS3SdqgXEPyCUDA1R0OKwYhySIiRsPrgf9HNTj/DmDPVZxCHV0o3VAREVErLYuIiKg1pid0mzx5snt6ejodRkTEmHLdddc9YHtK/ZYrjOlk0dPTw/z58zsdRkTEmCLp7vqtVpZuqIiIqJVkERERtZIsIiKiVpJFRETUSrKIiIhaSRYREVErySIiImolWURERK0ki4iIqJVkEcPSM/siemZf1OkwIqLNkiwiIqJWkkVERNRKsoiuNNTurXSLRbRHkkVERNRKsoiIiFpJFhERUSvJIiIiaiVZRERErSSLiIiolWQRERG1kiwiIqJWkkWMGbngLqJzkiwiIqJW25KFpFMlLZF0S0vZFyT9TtLNkn4gaYOWdUdLukPS7ZLe1q64IiJi8NrZsjgd2KVP2cXAtrZfBvw3cDSApG2AfYCXln1OkjSpjbFFRMQgtC1Z2L4CeKhP2c9tLy+LVwPTyus9gO/aftL2XcAdwGvbFVtERAxOJ8csDgJ+Ul5PBe5tWbeolD2LpEMlzZc0f+nSpW0OMSIioEPJQtIxwHLgzN6ifjZzf/vanmt7pu2ZU6ZMaVeIERHRYrXRrlDSLGB3YCfbvQlhEbB5y2bTgPtGO7YYP3pPsV04Z7cORxIxPoxqy0LSLsDHgXfa/lPLqguAfSStIWlLYAZw7WjGFhERA2tby0LSWcCOwGRJi4DjqM5+WgO4WBLA1bY/aPtWSecAv6Xqnvqw7afbFVtERAxO25KF7X37KT5lFdufAJzQrniie7RehZ1uooixIVdwR0RErSSLiIiolWQRERG1kiwiIqJWkkVERNQa9YvyIlal3feryJlYEUOTlkVERNRKsoiIiFpJFhERUSvJIiIiamWAO7pCuwe2I2J40rKIMadn9kVJLhGjLMkiIiJqpRsqxry0MiLaLy2LiIiolWQRERG1kiwiIqJWkkVERNRKsoiIiFpJFhERUSvJIiIiauU6i5gQmlyL0btN7nMR8WxpWURERK22tSwknQrsDiyxvW0p2wg4G+gBFgJ72364rDsaOBh4Gjjc9s/aFVt0Rn/f7nP1dcTY0M6WxenALn3KZgOX2J4BXFKWkbQNsA/w0rLPSZImtTG2iIgYhLYlC9tXAA/1Kd4DmFdezwP2bCn/ru0nbd8F3AG8tl2xRUTE4Iz2APcmthcD2F4saeNSPhW4umW7RaXsWSQdChwKMH369DaGGhNFusIi6nXL2VDqp8z9bWh7LjAXYObMmf1uExNDPuQjRs9onw11v6RNAcrzklK+CNi8ZbtpwH2jHFtERAxgtJPFBcCs8noWcH5L+T6S1pC0JTADuHaUY4uIiAG089TZs4AdgcmSFgHHAXOAcyQdDNwD7AVg+1ZJ5wC/BZYDH7b9dLtii4iIwWlbsrC97wCrdhpg+xOAE9oVT0REDF2u4I626Zl9UQahI8aJQSULSRtKelm7gomIiO5UmywkXS5pvTJVx03AaZJObH9oERHRLZqMWaxv+zFJhwCn2T5O0s3tDiw6rxtmYR1uN1a6wSJGRpNuqNXKNRF7Axe2OZ6IiOhCTZLFZ4CfAXfY/o2krYDftzesiIjoJrXdULbPBc5tWb4TeFc7g4qIiO5SmywkTQE+QHUPime2t31Q+8KKiIhu0mSA+3zgV8AvqG5MFBERE0yTZLGW7Y+3PZKIiOhaTQa4L5S0a9sjiYiIrtUkWXyEKmH8RdJjkpZJeqzdgUVERPdocjbUuqMRSEREdK8m031I0n6SPlmWN5eU+2NHREwgTbqhTgJeD7y3LD8OfL1tEUVERNdpcjbU62y/StINALYflrR6m+OKiIgu0qRl8ZSkSYDhmYv0/tbWqCK6VO7RERNVk5bFV4AfABtLOgF4N3BsW6OK6KDWZNDJGXcjukmTZHEecB3V7VAF7Anc38aYIiKiyzRJFt8H9rT9O4AyXfnFwKvbGVh0j3zTjogmYxY/BM6VNElSD9V05Ue3M6iIiOguTS7K+2Y5++mHVDPP/pPtK9scV0REdJEBk4WkI1sXgc2BG4HtJG1ne8j34ZZ0BHAI1RlWC4ADgbWAs6kS0kJgb9sPD7WOiIgYOavqhlq35bEO1RlRd7SUDYmkqcDhwEzb2wKTgH2A2cAltmcAl5TliIjoAgO2LGwf37osad2q2I+PUL3Pk/QUVYviPqpxkB3L+nnA5UCmRo+I6AJN7pS3LXAGsFFZfgB4v+1bh1Kh7T9I+iJwD/Bn4Oe2fy5pE9uLyzaLJW08QDyHAocCTJ8+fSghRI2JctHZRPk5I0ZCk7Oh5gJH2t7C9hbAUcA3h1qhpA2BPYAtgc2AtSXt13R/23Ntz7Q9c8qUKUMNIyIiBqFJsljb9mW9C7YvB9YeRp07A3fZXmr7KarrON4A3F+u4ei9lmPJMOqIiIgR1CRZ3Cnpk5J6yuNY4K5h1HkP1RlVa0kS1ZXhtwEXALPKNrOo7v0dXSZzI0VMTE2u4D4IOJ6qBQBwBXDAUCu0fY2k84DrgeXADVRdXesA50g6mCqh7DXUOiIiYmQ1SRY72z68tUDSXsC5Q63U9nHAcX2Kn6RqZURERJdp0g3V39Qeme4jImICWdUV3G8HdgWmSvpKy6r1qLqPIsa93vGZTKAYE92quqHuA+YD76SaorzXMuCIdgYV0W0yqB8T3aqu4L4JuEnSd8oprhERMUHVjlkkUURERJMB7oiImOAGTBaSzijPHxm9cCIiohutqmXxaklbAAdJ2lDSRq2P0QowIiI6b1VnQ30D+CmwFdXZUGpZ51IeERETwIAtC9tfsf0S4FTbW9nesuWRRBERMYE0uQf3P0t6OfDGUnSF7ZvbG1Z0u1x3EDGx1J4NJelw4Exg4/I4U9Jh7Q4sIiK6R5OJBA8BXmf7CQBJnweuAr7azsAiIqJ7NLnOQsDTLctPs/Jgd0REjHNNWhanAddI+kFZ3hM4pW0RRURE12kywH2ipMuBHahaFAfavqHdgcXoaB2ozsyqETGQJi0LbF9PdWe7iIiYgDI3VERE1EqyiIiIWqtMFpImSfrFaAUTERHdaZXJwvbTwJ8krT9K8URERBdqMsD9F2CBpIuBJ3oLbR/etqgixoicTRYTRZNkcVF5RETEBNXkOot5kp4HTLd9+0hUKmkD4GRgW6rpzg8CbgfOBnqAhcDeth8eifoiImJ4mkwk+A7gRqp7WyDpFZIuGGa9XwZ+antr4OXAbcBs4BLbM4BLynK0Sc/sizJzbEQ01uTU2U8DrwUeAbB9I7DlUCuUtB7wD5QpQ2z/1fYjwB7AvLLZPKppRSIiogs0SRbLbT/ap8zDqHMrYClwmqQbJJ0saW1gE9uLAcrzxv3tLOlQSfMlzV+6dOkwwoiIiKaaJItbJL0XmCRphqSvAlcOo87VgFcB/2H7lVRnWDXucrI91/ZM2zOnTJkyjDAiIqKpJsniMOClwJPAWcBjwEeHUeciYJHta8ryeVTJ435JmwKU5yXDqCOirTLmExNNk7Oh/gQcU256ZNvLhlOh7T9KulfSi8vZVTsBvy2PWcCc8nz+cOqJGG29ySPXW8R4VJssJL0GOBVYtyw/Chxk+7ph1HsY1e1ZVwfuBA6kauWcI+lg4B5gr2EcPyIiRlCTi/JOAT5k+1cAknaguiHSy4ZaaTmjamY/q3Ya6jGje6W7JmLsazJmsaw3UQDY/jUwrK6oGH/Shx8xvg3YspD0qvLyWkn/STW4beA9wOXtDy0iIrrFqrqh/r3P8nEtr4dznUWMY2ldRIxPAyYL228ezUAiIqJ7NTkbagPg/VQT/D2zfaYoj4iYOJqcDfVj4GpgAfC39oYTERHdqEmyWNP2kW2PJCIiulaTU2fPkPQBSZtK2qj30fbIIiKiazRpWfwV+AJwDCvOgjLV7LERETEBNEkWRwIvtP1Au4OJiIju1KQb6lbgT+0OJGK8yNXsMR41aVk8Ddwo6TKqacqBnDobETGRNEkWPyyPiIiYoJrcz2Je3TYRETG+NbmC+y76mQvKds6GioiYIJp0Q7Xed2JNqpsS5TqLiIZaB7tzF70Yq5p0Qz3Yp+hLkn4NfKo9IUWMDzkjKsaTJt1Qr2pZfA5VS2PdtkUUERFdp0k3VOt9LZYDC4G92xJNRER0pSbdULmvRUTEBNekG2oN4F08+34Wn2lfWBER0U2adEOdDzwKXEfLFdwRETFxNEkW02zvMtIVS5oEzAf+YHv3Mu352VQtmIXA3rYfHul6Izqp9wypnEIbY02TiQSvlPT3baj7I8BtLcuzgUtszwAuKcsREdEFmiSLHYDrJN0u6WZJCyTdPJxKJU0DdgNObineA+idWmQesOdw6oiIiJHTpBvq7W2o90vAx1j5eo1NbC8GsL1Y0sZtqDciIoagyamzd49khZJ2B5bYvk7SjkPY/1DgUIDp06ePZGgRETGAJt1QI2174J2SFgLfBf5R0reB+yVtClCel/S3s+25tmfanjllypTRijkiYkIb9WRh+2jb02z3APsAl9reD7gAmFU2m0V1ym5ERHSBTrQsBjIHeIuk3wNvKcsREdEFmgxwt43ty4HLy+sHgZ06GU9ERPSvm1oWERHRpTrasojRl3ssRMRQpGURERG1kiwiIqJWkkVERNRKsoiIiFpJFhERUSvJIiIiaiVZRERErSSLiIiolWQRERG1kiwiIqJWkkVEB/XMvihTsMSYkGQRERG1MpFgRAekNRFjTVoWERFRK8kiIiJqJVlEdJm+g94ZBI9ukGQRERG1kiwiIqJWkkVERNRKsoiIiFpJFhERUWvUL8qTtDnwLeD5wN+Auba/LGkj4GygB1gI7G374dGOL6ITcrZTdLtOtCyWA0fZfgmwHfBhSdsAs4FLbM8ALinLERHRBUY9WdhebPv68noZcBswFdgDmFc2mwfsOdqxRURE/zo6ZiGpB3glcA2wie3FUCUUYOMB9jlU0nxJ85cuXTpqsUZ0Wi7Oi07qWLKQtA7wPeCjth9rup/tubZn2p45ZcqU9gUYERHP6Miss5KeS5UozrT9/VJ8v6RNbS+WtCmwpBOxRXSLtCKim3TibCgBpwC32T6xZdUFwCxgTnk+f7RjixgLWpPIwjm7dTCSmEg60bLYHtgfWCDpxlL2CaokcY6kg4F7gL06EFtERPRj1JOF7V8DGmD1TqMZS0RENJMruCMiolaSRURE1Mo9uMexDISOf72/4/x+o93SsoiIiFpJFhHjQK7ujnZLsoiIiFpJFhERUSvJImIcSXdUtEuSRURE1EqyiIiIWkkWERFRK8kiIiJqJVlEREStJIuIcS5nSMVISLKIiIhaSRYREVErs85GjEND7XbKLLYxkLQsIiKiVpLFOJKBzFiVvn8f+XuJwUg31BiR7oEYKUkQMRRpWURERK0ki4gYULqqoleSRURE1Oq6MQtJuwBfBiYBJ9ue0+GQIsa1/loOfctalzNuNjF1VbKQNAn4OvAWYBHwG0kX2P5tJ+IZ7UHlwf5DZtA7ut1g/kbz99zduq0b6rXAHbbvtP1X4LvAHh2OKSJiwpPtTsfwDEnvBnaxfUhZ3h94ne1/adnmUODQsrgtcMuoB9qdJgMPdDqILpH3YoW8FyvkvVjhxbbXHcwOXdUNBaifspWyme25wFwASfNtzxyNwLpd3osV8l6skPdihbwXK0iaP9h9uq0bahGwecvyNOC+DsUSERFFtyWL3wAzJG0paXVgH+CCDscUETHhdVU3lO3lkv4F+BnVqbOn2r51FbvMHZ3IxoS8FyvkvVgh78UKeS9WGPR70VUD3BER0Z26rRsqIiK6UJJFRETUGrPJQtIukm6XdIek2Z2Op1MkbS7pMkm3SbpV0kc6HVMnSZok6QZJF3Y6lk6TtIGk8yT9rvx9vL7TMXWKpCPK/8ctks6StGanYxotkk6VtETSLS1lG0m6WNLvy/OGdccZk8miZVqQtwPbAPtK2qazUXXMcuAo2y8BtgM+PIHfC4CPALd1Oogu8WXgp7a3Bl7OBH1fJE0FDgdm2t6W6uSZfTob1ag6HdilT9ls4BLbM4BLyvIqjclkQaYFeYbtxbavL6+XUX0gTO1sVJ0haRqwG3Byp2PpNEnrAf8AnAJg+6+2H+loUJ21GvA8SasBazGBrt+yfQXwUJ/iPYB55fU8YM+644zVZDEVuLdleRET9AOylaQe4JXANR0OpVO+BHwM+FuH4+gGWwFLgdNKt9zJktbudFCdYPsPwBeBe4DFwKO2f97ZqDpuE9uLofrCCWxct8NYTRa104JMNJLWAb4HfNT2Y52OZ7RJ2h1YYvu6TsfSJVYDXgX8h+1XAk/QoKthPCr98XsAWwKbAWtL2q+zUY09YzVZZFqQFpKeS5UozrT9/U7H0yHbA++UtJCqW/IfJX27syF11CJgke3eVuZ5VMljItoZuMv2UttPAd8H3tDhmDrtfkmbApTnJXU7jNVkkWlBCkmi6pe+zfaJnY6nU2wfbXua7R6qv4dLbU/Yb4+2/wjcK+nFpWgnoCP3hekC9wDbSVqr/L/sxAQd7G9xATCrvJ4FnF+3Q1dN99HUEKYFGc+2B/YHFki6sZR9wvaPOxdSdInDgDPLF6o7gQM7HE9H2L5G0nnA9VRnD97ABJr6Q9JZwI7AZEmLgOOAOcA5kg6mSqZ71R4n031ERESdsdoNFRERoyjJIiIiaiVZRERErSSLiIiolWQRERG1kixizJL0eBuO+QpJu7Ysf1rSvw7jeHuVGV8v61PeI+m9DfY/QNLXhlp/xEhJsohY2SuAXes2GoSDgQ/ZfnOf8h6gNllEdIskixgXJP0vSb+RdLOk40tZT/lW/81yL4OfS3peWfeasu1Vkr5Q7nOwOvAZ4D2SbpT0nnL4bSRdLulOSYcPUP++khaU43y+lH0K2AH4hqQv9NllDvDGUs8RktaUdFo5xg2S+iYXJO1W4p0s6a3l9fWSzi1zgyFpoaTjS/kCSVuX8jeVum4sx1932G96TCy288hjTD6Ax8vzW6muyBXVF6ALqabn7qG6YvcVZbtzgP3K61uAN5TXc4BbyusDgK+11PFp4EpgDWAy8CDw3D5xbEZ1FewUqlkRLgX2LOsup7qPQt/YdwQubFk+CjitvN66HG/N3niA/wH8CtiwxHEFsHbZ/uPAp8rrhcBh5fWHgJPL6x8B25fX6wCrdfr3l8fYeqRlEePBW8vjBqopHbYGZpR1d9m+sby+DuiRtAGwru0rS/l3ao5/ke0nbT9ANeHaJn3Wvwa43NVEdcuBM6mS1WDsAJwBYPt3wN3Ai8q6N1MlhN1sP0x1k6ttgP8qU7zMArZoOVbvZJLXUSVMgP8CTiwtow1KnBGNjcm5oSL6EPA52/+5UmF1f48nW4qeBp5H/1Pcr0rfY/T9vxns8fqzqmPcSXV/ihcB88u2F9ved4Dte+N9JlbbcyRdRDUec7WknUtSimgkLYsYD34GHNTSbz9V0oA3cynfzpdJ2q4Utd5icxkw2P78a4A3lbGEScC+wC9r9ulbzxXA+0r8LwKmA7eXdXcD/xP4lqSXAlcD20t6Ydl+rbLPgCS9wPYC25+nSjhbD+YHjEiyiDHP1V3PvgNcJWkB1b0b6j7wDwbmSrqK6pv6o6X8MqoB7dYB7rr6FwNHl31vAq63XTfl883Ackk3SToCOAmYVOI/GzjA9jMtGtu3UyWTc4H1qMYyzpJ0M1XyqPvw/2gZfL8J+DPwkyY/W0SvzDobE5KkdWw/Xl7PBja1/ZEOhxXRtTJmERPVbpKOpvofuJvqm3pEDCAti4iIqJUxi4iIqJVkERERtZIsIiKiVpJFRETUSrKIiIha/x+zs+JkUOPZ6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向tokenize**  \n",
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "#             text = text + cn_model.index2word[i]\n",
    "            text = text + cn_model.index_to_key[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下可见，训练样本的极性并不是那么精准，比如说下面的样本，对早餐并不满意，但被定义为正面评价，这会迷惑我们的模型，不过我们暂时不对训练样本进行任何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备Embedding Matrix**  \n",
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。  \n",
    "注意我们只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前20000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index_to_key[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index_to_key[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding（填充）和truncating（修剪）**  \n",
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         290,  3053,    57,   169,    73,     1,    25, 11216,    49,\n",
       "         163, 15985,     0,     0,    30,     8,     0,     1,   228,\n",
       "         223,    40,    35,   653,     0,     5,  1642,    29, 11216,\n",
       "        2751,   500,    98,    30,  3159,  2225,  2146,   371,  6285,\n",
       "         169, 27396,     1,  1191,  5432,  1080, 20055,    57,   562,\n",
       "           1, 22671,    40,    35,   169,  2567,     0, 42665,  7761,\n",
       "         110,     0,     0, 41281,     0,   110,     0, 35891,   110,\n",
       "           0, 28781,    57,   169,  1419,     1, 11670,     0, 19470,\n",
       "           1,     0,     0,   169, 35071,    40,   562,    35, 12398,\n",
       "         657,  4857], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前2000样本为1，后2000为0\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                        房间很大还有海景阳台走出酒店就是沙滩非常不错唯一遗憾的就是不能刷 不方便\n",
      "class:  1\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[35]))\n",
    "print('class: ',y_train[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。   \n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$\n",
    "输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 05:30:35.528786: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-30 05:30:35.530951: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting numpy==1.20.0\n",
      "  Using cached numpy-1.20.0-cp37-cp37m-macosx_10_9_x86_64.whl (16.0 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.0 requires numpy~=1.19.2, but you have numpy 1.20.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.20.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U numpy==1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install python==3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.19.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting tensorflow==2.4.0\n",
      "  Downloading tensorflow-2.4.0-cp37-cp37m-macosx_10_11_x86_64.whl (175.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 175.4 MB 10.2 MB/s eta 0:00:01  |█▍                              | 7.3 MB 7.2 MB/s eta 0:00:24     |██████████████▍                 | 79.0 MB 3.1 MB/s eta 0:00:32     |████████████████████▍           | 111.5 MB 5.3 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (3.3.0)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 28.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard~=2.4 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (2.9.1)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (0.37.1)\n",
      "Collecting h5py~=2.10.0\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl (3.0 MB)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (1.1.2)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (1.12)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (3.19.4)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow==2.4.0) (1.1.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.0.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (61.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.8.0)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-macosx_10_7_x86_64.whl size=33443 sha256=2cf0314cc67d869c81b0f578a1c6c8abdf3bd8b4c278368281d0dd902038c82f\n",
      "  Stored in directory: /Users/shao/Library/Caches/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built wrapt\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: typing-extensions, six, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: six\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: grpcio\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: grpcio 1.47.0\n",
      "    Uninstalling grpcio-1.47.0:\n",
      "      Successfully uninstalled grpcio-1.47.0\n",
      "  Attempting uninstall: absl-py\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: absl-py 1.1.0\n",
      "    Uninstalling absl-py-1.1.0:\n",
      "      Successfully uninstalled absl-py-1.1.0\n",
      "  Attempting uninstall: wrapt\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: wrapt 1.13.3\n",
      "    Uninstalling wrapt-1.13.3:\n",
      "      Successfully uninstalled wrapt-1.13.3\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: h5py\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: h5py 3.6.0\n",
      "    Uninstalling h5py-3.6.0:\n",
      "      Successfully uninstalled h5py-3.6.0\n",
      "  Attempting uninstall: gast\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "Successfully installed absl-py-0.15.0 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 six-1.15.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (2.0.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.9.1-cp37-cp37m-macosx_10_14_x86_64.whl (228.4 MB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (4.1.1)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.1-py2.py3-none-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp37-cp37m-macosx_10_10_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.4-cp37-cp37m-macosx_10_9_x86_64.whl (960 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: packaging in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: setuptools in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (61.2.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.9.0-py2.py3-none-any.whl (167 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.3)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Using cached charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/shao/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, grpcio, google-auth-oauthlib, absl-py, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, libclang, keras, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.14.1\n",
      "    Uninstalling grpcio-1.14.1:\n",
      "      Successfully uninstalled grpcio-1.14.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.0.0\n",
      "    Uninstalling tensorflow-estimator-2.0.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.0.0\n",
      "    Uninstalling tensorboard-2.0.0:\n",
      "      Successfully uninstalled tensorboard-2.0.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.0.0\n",
      "    Uninstalling tensorflow-2.0.0:\n",
      "      Successfully uninstalled tensorflow-2.0.0\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.20.0\n",
      "  Using cached numpy-1.20.0-cp37-cp37m-macosx_10_9_x86_64.whl (16.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "Successfully installed numpy-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U numpy==1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (bidirectional_1/forward_lstm_2/strided_slice:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wl/0x1mzt_901j9znqzbv5pn4p40000gn/T/ipykernel_62002/2753884906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 结果模型训练的效果没有去年最早的时候效果好了,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 有兴趣的同学可以调整一下模型参数, 看看会不会有更好的结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       y = self.forward_layer(forward_inputs,\n\u001b[0;32m--> 642\u001b[0;31m                              initial_state=forward_state, **kwargs)\n\u001b[0m\u001b[1;32m    643\u001b[0m       y_rev = self.backward_layer(backward_inputs,\n\u001b[1;32m    644\u001b[0m                                   initial_state=backward_state, **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   2547\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_recurrent_dropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m     return super(LSTM, self).call(\n\u001b[0;32m-> 2549\u001b[0;31m         inputs, mask=mask, training=training, initial_state=initial_state)\n\u001b[0m\u001b[1;32m   2550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    680\u001b[0m            constants=None):\n\u001b[1;32m    681\u001b[0m     inputs, initial_state, constants = self._process_inputs(\n\u001b[0;32m--> 682\u001b[0;31m         inputs, initial_state, constants)\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       init_state = get_initial_state_fn(\n\u001b[0;32m--> 606\u001b[0;31m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m       init_state = _generate_zero_filled_state(batch_size, self.cell.state_size,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2312\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m     return list(_generate_zero_filled_state_for_cell(\n\u001b[0;32m-> 2314\u001b[0;31m         self, inputs, batch_size, dtype))\n\u001b[0m\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2750\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2752\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m   2766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2768\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2769\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2770\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m   2763\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2765\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2347\u001b[0m         \u001b[0;31m# Create a constant if it won't be very big. Otherwise create a fill op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m         \u001b[0;31m# to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2304\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2305\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2306\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2307\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2308\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 3052\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DeepLearning/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (bidirectional_1/forward_lstm_2/strided_slice:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "# 在2019年6月10日修改了一些大坑的bug, 可能是数据的顺序变了, \n",
    "# 结果模型训练的效果没有去年最早的时候效果好了, \n",
    "# 有兴趣的同学可以调整一下模型参数, 看看会不会有更好的结果\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**  \n",
    "我在这个教程中尝试了几种神经网络结构，因为训练样本比较少，所以我们可以尽情尝试，训练过程等待时间并不长：  \n",
    "**GRU：**如果使用GRU的话，测试样本可以达到87%的准确率，但我测试自己的文本内容时发现，GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。  \n",
    "**BiLSTM：**测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆，有兴趣的朋友可以深入研究一下。  \n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU的代码\n",
    "# model.add(GRU(units=32, return_sequences=True))\n",
    "# model.add(GRU(units=16, return_sequences=True))\n",
    "# model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 236, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 236, 128)          186880    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,196,177\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (300, 256) and (300, 128) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3240 samples, validate on 360 samples\n",
      "Epoch 1/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 0.65662, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 34s 10ms/sample - loss: 0.6846 - accuracy: 0.5698 - val_loss: 0.6566 - val_accuracy: 0.6639\n",
      "Epoch 2/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.6266 - accuracy: 0.6562\n",
      "Epoch 00002: val_loss improved from 0.65662 to 0.56397, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 30s 9ms/sample - loss: 0.6266 - accuracy: 0.6556 - val_loss: 0.5640 - val_accuracy: 0.7139\n",
      "Epoch 3/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.5093 - accuracy: 0.7591\n",
      "Epoch 00003: val_loss improved from 0.56397 to 0.51803, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.5100 - accuracy: 0.7583 - val_loss: 0.5180 - val_accuracy: 0.7556\n",
      "Epoch 4/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4914 - accuracy: 0.7734\n",
      "Epoch 00004: val_loss improved from 0.51803 to 0.43727, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.4904 - accuracy: 0.7744 - val_loss: 0.4373 - val_accuracy: 0.8250\n",
      "Epoch 5/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4549 - accuracy: 0.8006\n",
      "Epoch 00005: val_loss did not improve from 0.43727\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3240/3240 [==============================] - 30s 9ms/sample - loss: 0.4564 - accuracy: 0.7994 - val_loss: 0.4508 - val_accuracy: 0.8000\n",
      "Epoch 6/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8206\n",
      "Epoch 00006: val_loss did not improve from 0.43727\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3240/3240 [==============================] - 30s 9ms/sample - loss: 0.4260 - accuracy: 0.8210 - val_loss: 0.4374 - val_accuracy: 0.8139\n",
      "Epoch 7/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4158 - accuracy: 0.8256\n",
      "Epoch 00007: val_loss improved from 0.43727 to 0.43676, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 31s 9ms/sample - loss: 0.4160 - accuracy: 0.8256 - val_loss: 0.4368 - val_accuracy: 0.8139\n",
      "Epoch 8/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8222\n",
      "Epoch 00008: val_loss improved from 0.43676 to 0.43648, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 31s 9ms/sample - loss: 0.4142 - accuracy: 0.8241 - val_loss: 0.4365 - val_accuracy: 0.8139\n",
      "Epoch 9/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4125 - accuracy: 0.8241\n",
      "Epoch 00009: val_loss improved from 0.43648 to 0.43615, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 31s 9ms/sample - loss: 0.4131 - accuracy: 0.8228 - val_loss: 0.4361 - val_accuracy: 0.8139\n",
      "Epoch 10/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8241\n",
      "Epoch 00010: val_loss improved from 0.43615 to 0.43576, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.4120 - accuracy: 0.8247 - val_loss: 0.4358 - val_accuracy: 0.8167\n",
      "Epoch 11/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4110 - accuracy: 0.8253\n",
      "Epoch 00011: val_loss improved from 0.43576 to 0.43573, saving model to sentiment_checkpoint.keras\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "3240/3240 [==============================] - 31s 10ms/sample - loss: 0.4109 - accuracy: 0.8253 - val_loss: 0.4357 - val_accuracy: 0.8167\n",
      "Epoch 12/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4112 - accuracy: 0.8241\n",
      "Epoch 00012: val_loss improved from 0.43573 to 0.43573, saving model to sentiment_checkpoint.keras\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 13/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8238\n",
      "Epoch 00013: val_loss improved from 0.43573 to 0.43572, saving model to sentiment_checkpoint.keras\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "3240/3240 [==============================] - 31s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 14/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4098 - accuracy: 0.8250\n",
      "Epoch 00014: val_loss improved from 0.43572 to 0.43572, saving model to sentiment_checkpoint.keras\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "3240/3240 [==============================] - 31s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 15/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4090 - accuracy: 0.8253\n",
      "Epoch 00015: val_loss improved from 0.43572 to 0.43572, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 30s 9ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 16/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8247\n",
      "Epoch 00016: val_loss did not improve from 0.43572\n",
      "3240/3240 [==============================] - 31s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 17/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4108 - accuracy: 0.8244\n",
      "Epoch 00017: val_loss did not improve from 0.43572\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 18/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4107 - accuracy: 0.8247\n",
      "Epoch 00018: val_loss improved from 0.43572 to 0.43572, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 19/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4080 - accuracy: 0.8263\n",
      "Epoch 00019: val_loss improved from 0.43572 to 0.43572, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n",
      "Epoch 20/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.4115 - accuracy: 0.8234\n",
      "Epoch 00020: val_loss did not improve from 0.43572\n",
      "3240/3240 [==============================] - 32s 10ms/sample - loss: 0.4102 - accuracy: 0.8247 - val_loss: 0.4357 - val_accuracy: 0.8194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e1c2885c18>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论**  \n",
    "我们首先对测试样本进行预测，得到了还算满意的准确度。  \n",
    "之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/sample - loss: 0.4799 - accuracy: 0.76750s - loss: 0.4699 - accuracy\n",
      "Accuracy:76.75%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "是一例正面评价 output=0.63\n",
      "酒店卫生条件非常不好\n",
      "是一例负面评价 output=0.47\n",
      "床铺非常舒适\n",
      "是一例正面评价 output=0.79\n",
      "房间很凉，不给开暖气\n",
      "是一例正面评价 output=0.53\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例正面评价 output=0.79\n",
      "酒店环境不好，住宿体验很不好\n",
      "是一例负面评价 output=0.48\n",
      "房间隔音不到位\n",
      "是一例负面评价 output=0.31\n",
      "晚上回来发现没有打扫卫生\n",
      "是一例正面评价 output=0.50\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "是一例负面评价 output=0.47\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**错误分类的文本**\n",
    "经过查看，发现错误分类的文本的含义大多比较含糊，就算人类也不容易判断极性，如index为101的这个句子，好像没有一点满意的成分，但这例子评价在训练样本中被标记成为了正面评价，而我们的模型做出的负面评价的预测似乎是合理的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                        由于2007年 有一些新问题可能还没来得及解决我因为工作需要经常要住那里所以慎重的提出以下 ：1 后 的 淋浴喷头的位置都太高我换了房间还是一样很不好用2 后的一些管理和服务还很不到位尤其是前台入住和 时代效率太低每次 都超过10分钟好像不符合 宾馆的要求\n",
      "预测的分类 0\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx=101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                 还是很 设施也不错但是 和以前 比急剧下滑了 和客房 的服务极差幸好我不是很在乎\n",
      "预测的分类 1\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
